## **Auto Correct and Prompt Validation Before AI Execution**

¿Cómo se podría implementar una capa de preprocesamiento de prompts que identifique y corrija errores gramaticales, consultas incompletas o entradas poco claras? ¿Cómo ayudas a los usuarios señalando el lenguaje dañino o sensible en sus prompts y sugiriendo alternativas seguras y éticas? Al mismo tiempo, ¿puedes mejorar la claridad y precisión de los prompts para mejorar la calidad de la salida de la IA? Para este proyecto, diseña un sistema que autocorrija y valide las entradas de los usuarios antes de enviarlas a la IA, asegurando que los prompts estén optimizados, sean conformes y estén libres de riesgos potenciales (por ejemplo, sesgo, lenguaje dañino o datos sensibles). Las soluciones deben ser efectivas en la identificación y corrección de errores o ambigüedades en los prompts. Es importante tener sensibilidad y especificidad en la detección de lenguaje dañino o riesgoso, y que haya una mejora en la calidad y relevancia de las respuestas de la IA después de la corrección del prompt.

**Resources**

[Scott and Mark learn responsible AI](https://www.youtube.com/watch?v=oFsLRtOuRyg)

[General availability of Prompt Shields in Azure AI Content Safety and Azure OpenAI Service](https://techcommunity.microsoft.com/blog/azure-ai-services-blog/general-availability-of-prompt-shields-in-azure-ai-content-safety-and-azure-open/4235560)

[Prompt Shields](https://learn.microsoft.com/en-us/azure/ai-services/content-safety/concepts/jailbreak-detection)

[Prompt flow in Azure AI Foundry portal](https://learn.microsoft.com/en-us/azure/ai-foundry/how-to/prompt-flow)

[Prompt flow repo](https://github.com/microsoft/promptflow/tree/main)

[Tune prompts using variants in Azure AI Foundry portal](https://learn.microsoft.com/en-us/azure/ai-foundry/how-to/flow-tune-prompts-using-variants)

[Harm categories in Azure AI Content Safety](https://learn.microsoft.com/en-us/azure/ai-services/content-safety/concepts/harm-categories?tabs=warning)
